import os
import json
import requests
import arxiv
from datetime import datetime, timedelta, timezone
import argparse

class PaperAnalyzer:
    def __init__(self):
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.base_url = os.getenv('OPENAI_BASE_URL', 'https://api.deepseek.com/v1')
        self.model = os.getenv('OPENAI_MODEL', 'deepseek-reasoner')
        # è®ºæ–‡å¯ä»¥ç”¨å•ç‹¬æ¨é€æ¸ é“ï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨è‚¡ç¥¨åŒä¸€ä¸ª
        self.feishu_url = os.getenv('PAPER_FEISHU_URL') or os.getenv('FEISHU_WEBHOOK_URL')
        
    def fetch_recent_papers(self, keywords, max_results=3):
        """æŠ“å–æœ€è¿‘24å°æ—¶çš„è®ºæ–‡"""
        client = arxiv.Client()
        papers = []
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        
        for keyword in keywords:
            search = arxiv.Search(
                query=keyword,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            for result in client.results(search):
                # åªå–æœ€è¿‘24å°æ—¶çš„
                if result.published >= yesterday:
                    papers.append({
                        'title': result.title,
                        'authors': [str(a) for a in result.authors[:3]],
                        'summary': result.summary,
                        'pdf_url': result.pdf_url,
                        'published': result.published.strftime('%Y-%m-%d'),
                        'categories': result.categories,
                        'keyword': keyword
                    })
        return papers

    def analyze_with_ai(self, paper):
        """ç”¨ DeepSeek åˆ†æè®ºæ–‡"""
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šå­¦æœ¯åŠ©æ‰‹ï¼Œè¯·å¿«é€Ÿè§£è¯»è¿™ç¯‡è®ºæ–‡ï¼Œè¾“å‡ºæ ¼å¼ä¸¥æ ¼å¦‚ä¸‹ï¼š

ğŸ“Œ **æ ‡é¢˜**ï¼š{paper['title']}
ğŸ‘¤ **ä½œè€…**ï¼š{', '.join(paper['authors'])}
ğŸ·ï¸ **é¢†åŸŸ**ï¼š{paper['categories'][0] if paper['categories'] else 'æœªåˆ†ç±»'}

ğŸ” **ä¸€å¥è¯æ€»ç»“**ï¼šï¼ˆç”¨ä¸­æ–‡ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒåˆ›æ–°ï¼Œä¸è¶…è¿‡50å­—ï¼‰

ğŸ’¡ **å…³é”®äº®ç‚¹**ï¼š
- æ–¹æ³•ï¼šï¼ˆå…³é”®æŠ€æœ¯/æ–¹æ³•ï¼‰
- ç»“æœï¼šï¼ˆä¸»è¦æ€§èƒ½æå‡æˆ–å‘ç°ï¼‰
- æ„ä¹‰ï¼šï¼ˆå¯¹å­¦æœ¯ç•Œ/å·¥ä¸šç•Œçš„ä»·å€¼ï¼‰

âš ï¸ **é€‚åˆäººç¾¤**ï¼šï¼ˆå¦‚ï¼šæ¨èNLPç ”ç©¶è€…å…³æ³¨/é€‚åˆæ¨èç³»ç»Ÿæ–¹å‘/å¯è·³è¿‡ç­‰ï¼‰

åŸå§‹æ‘˜è¦ï¼š{paper['summary'][:1000]}
        """
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.model,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': 0.7
        }
        
        try:
            response = requests.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=data,
                timeout=120
            )
            result = response.json()
            return result['choices'][0]['message']['content']
        except Exception as e:
            return f"åˆ†æå¤±è´¥: {str(e)}"

    def send_feishu(self, content):
        """æ¨é€åˆ°é£ä¹¦"""
        if not self.feishu_url:
            print("æœªé…ç½®é£ä¹¦ Webhook")
            return
            
        formatted = f"""
ğŸ“ **æ¯æ—¥è®ºæ–‡é€Ÿé€’** | {datetime.now().strftime('%Y-%m-%d')}

{content}

---
ğŸ“š æ¥æºï¼šarXiv | ç”± DeepSeek-R1 åˆ†æ
        """
        
        payload = {
            "msg_type": "text",
            "content": {
                "text": formatted
            }
        }
        
        requests.post(self.feishu_url, json=payload)
        print("è®ºæ–‡æ¨é€æˆåŠŸ")

    def run(self, keywords_str, max_results):
        keywords = [k.strip() for k in keywords_str.split(',')]
        papers = self.fetch_recent_papers(keywords, max_results)
        
        if not papers:
            self.send_feishu("ğŸ“­ ä»Šæ—¥æš‚æ— æ–°è®ºæ–‡ï¼ˆæˆ–arXivæœªæ›´æ–°ï¼‰")
            return
            
        # å»é‡
        seen_titles = set()
        unique_papers = []
        for p in papers:
            if p['title'] not in seen_titles:
                seen_titles.add(p['title'])
                unique_papers.append(p)
        
        full_report = f"ğŸ“Š å…±å‘ç° {len(unique_papers)} ç¯‡æ–°è®ºæ–‡\n\n"
        
        for i, paper in enumerate(unique_papers, 1):
            print(f"æ­£åœ¨åˆ†æç¬¬ {i} ç¯‡: {paper['title'][:50]}...")
            analysis = self.analyze_with_ai(paper)
            full_report += f"â”â”â”â”â”â”â”â”â”â”â”â”\nã€{i}ã€‘{analysis}\n\n"
        
        self.send_feishu(full_report)
        print(f"å®Œæˆï¼åˆ†æäº† {len(unique_papers)} ç¯‡è®ºæ–‡")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--keywords', default='LLM,RAG,Agent', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--max', type=int, default=3, help='æ¯å…³é”®è¯æ•°é‡')
    args = parser.parse_args()
    
    analyzer = PaperAnalyzer()
    analyzer.run(args.keywords, args.max)
