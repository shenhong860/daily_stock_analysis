import os
import json
import requests
import arxiv
from datetime import datetime, timedelta, timezone
import argparse

class PaperAnalyzer:
    def __init__(self):
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.base_url = os.getenv('OPENAI_BASE_URL', 'https://api.deepseek.com/v1')
        self.model = os.getenv('OPENAI_MODEL', 'deepseek-reasoner')
        # è®ºæ–‡å¯ä»¥ç”¨å•ç‹¬æ¨é€æ¸ é“ï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨è‚¡ç¥¨åŒä¸€ä¸ª
        self.feishu_url = os.getenv('PAPER_FEISHU_URL') or os.getenv('FEISHU_WEBHOOK_URL')
        
    def fetch_recent_papers(self, keywords, max_results=3):
        """æŠ“å–æœ€è¿‘24å°æ—¶çš„è®ºæ–‡"""
        client = arxiv.Client()
        papers = []
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        
        for keyword in keywords:
            search = arxiv.Search(
                query=keyword,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            for result in client.results(search):
                # åªå–æœ€è¿‘24å°æ—¶çš„
                if result.published >= yesterday:
                    papers.append({
                        'title': result.title,
                        'authors': [str(a) for a in result.authors[:3]],
                        'summary': result.summary,
                        'pdf_url': result.pdf_url,
                        'published': result.published.strftime('%Y-%m-%d'),
                        'categories': result.categories,
                        'keyword': keyword
                    })
        return papers

            def analyze_with_ai(self, paper):
        """ç²¾ç®€ç‰ˆè®ºæ–‡åˆ†æï¼Œé€‚åˆå¿«é€Ÿé˜…è¯»"""
        # æ£€æµ‹æ˜¯å¦ä¸ºå•ç»†èƒ/ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸ
        categories = [c.lower() for c in paper['categories']]
        is_bio = any(x in str(categories) for x in ['bio', 'genomics', 'rna', 'cell', 'medical'])
        
        # æ ¹æ®é¢†åŸŸè°ƒæ•´å…³é”®è¯
        method_hint = "å•ç»†èƒ(scRNA-seq)æ³¨æ„dropout/æ‰¹æ¬¡æ•ˆåº”/é™ç»´è´¨é‡æ§åˆ¶" if is_bio else "å…³æ³¨æ·±åº¦å­¦ä¹ æ¶æ„/æŸå¤±å‡½æ•°è®¾è®¡/è®¡ç®—æ•ˆç‡"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½é«˜æ•ˆçš„å­¦æœ¯çŒæ‰‹ï¼Œè¯·ç”¨æç®€æ–¹å¼åˆ†æè¿™ç¯‡è®ºæ–‡ï¼Œæ¯éƒ¨åˆ†ä¸¥æ ¼é™åˆ¶å­—æ•°ï¼š

ã€è®ºæ–‡ã€‘{paper['title']}
ã€ä½œè€…ã€‘{', '.join(paper['authors'][:2])}ç­‰
ã€é¢†åŸŸã€‘{paper['categories'][0]}

æ‘˜è¦ï¼š{paper['summary'][:800]}...

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆæ€»å­—æ•°<600å­—ï¼‰ï¼š

ğŸ” **æ‘˜è¦ç¿»è¯‘**ï¼ˆ100å­—å†…ï¼‰ï¼š
å‡†ç¡®ç¿»è¯‘æ ¸å¿ƒè´¡çŒ®

ğŸ’¡ **ä¸ºä»€ä¹ˆåšï¼Ÿ**ï¼ˆç—›ç‚¹ï¼Œ50å­—å†…ï¼‰ï¼š
ç°æœ‰æ–¹æ³•ç¼ºé™·+æœ¬æ–‡è§£å†³æ€è·¯

âš™ï¸ **æ€ä¹ˆåšï¼Ÿ**ï¼ˆ Pipelineï¼Œ150å­—å†…ï¼‰ï¼š
1.è¾“å…¥â†’2.æ ¸å¿ƒæ­¥éª¤â†’3.è¾“å‡ºï¼Œé¿å…æœ¯è¯­å †ç Œï¼Œç”¨"åŠ¨è¯+å¯¹è±¡"æ ¼å¼

ğŸ“Š **å¥½åœ¨å“ªé‡Œï¼Ÿ**ï¼ˆ50å­—å†…ï¼‰ï¼š
å…³é”®æŒ‡æ ‡æå‡ï¼ˆå¦‚ARI+15%/é€Ÿåº¦x3å€ï¼‰vs SOTAæ–¹æ³•

âš ï¸ **å‘åœ¨å“ªï¼Ÿ**ï¼ˆ30å­—å†…ï¼‰ï¼š
è®¡ç®—å¼€é”€/æ•°æ®ä¾èµ–/å‚æ•°æ•æ„Ÿæ€§é—®é¢˜

ğŸ› ï¸ **å¤ç°éš¾åº¦**ï¼ˆ30å­—å†…ï¼‰ï¼š
å¼€æºï¼Ÿï¼ˆGitHub:æœ‰/æ— ï¼‰| ç¡¬ä»¶è¦æ±‚ | å…³é”®ä¾èµ–åŒ…

ğŸ¯ **é€Ÿè®°ç‰ˆ**ï¼ˆ20å­—å†…æ ¸å¿ƒ+3æ­¥Pipelineï¼‰ï¼š
æ ¸å¿ƒï¼š____ï¼ˆå¦‚"å›¾å·ç§¯å»æ‰¹æ¬¡"ï¼‰
3æ­¥ï¼š1.____ 2.____ 3.____ï¼ˆç”¨åˆä¸­è¯æ±‡æè¿°ï¼Œå‹¿ç”¨è®ºæ–‡ç”Ÿé€ è¯ï¼‰

é¢†åŸŸæç¤ºï¼š{method_hint}
"""
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.model,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': 0.4,
            'max_tokens': 1200
        }
        
        try:
            response = requests.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=data,
                timeout=90
            )
            result = response.json()
            analysis = result['choices'][0]['message']['content']
            
            return f"{analysis}\n\nğŸ“„ {paper['pdf_url']}"
            
        except Exception as e:
            return f"âŒ åˆ†æå¤±è´¥: {str(e)}"
        

    def send_feishu(self, content):
        """æ¨é€åˆ°é£ä¹¦"""
        if not self.feishu_url:
            print("æœªé…ç½®é£ä¹¦ Webhook")
            return
            
        formatted = f"""
ğŸ“ **æ¯æ—¥è®ºæ–‡é€Ÿé€’** | {datetime.now().strftime('%Y-%m-%d')}

{content}

---
ğŸ“š æ¥æºï¼šarXiv | ç”± DeepSeek-R1 åˆ†æ
        """
        
        payload = {
            "msg_type": "text",
            "content": {
                "text": formatted
            }
        }
        
        requests.post(self.feishu_url, json=payload)
        print("è®ºæ–‡æ¨é€æˆåŠŸ")

    def run(self, keywords_str, max_results):
        keywords = [k.strip() for k in keywords_str.split(',')]
        papers = self.fetch_recent_papers(keywords, max_results)
        
        if not papers:
            self.send_feishu("ğŸ“­ ä»Šæ—¥æš‚æ— æ–°è®ºæ–‡ï¼ˆæˆ–arXivæœªæ›´æ–°ï¼‰")
            return
            
        # å»é‡
        seen_titles = set()
        unique_papers = []
        for p in papers:
            if p['title'] not in seen_titles:
                seen_titles.add(p['title'])
                unique_papers.append(p)
        
        full_report = f"ğŸ“Š å…±å‘ç° {len(unique_papers)} ç¯‡æ–°è®ºæ–‡\n\n"
        
        for i, paper in enumerate(unique_papers, 1):
            print(f"æ­£åœ¨åˆ†æç¬¬ {i} ç¯‡: {paper['title'][:50]}...")
            analysis = self.analyze_with_ai(paper)
            full_report += f"â”â”â”â”â”â”â”â”â”â”â”â”\nã€{i}ã€‘{analysis}\n\n"
        
        self.send_feishu(full_report)
        print(f"å®Œæˆï¼åˆ†æäº† {len(unique_papers)} ç¯‡è®ºæ–‡")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--keywords', default='LLM,RAG,Agent', help='æœç´¢å…³é”®è¯')
    parser.add_argument('--max', type=int, default=3, help='æ¯å…³é”®è¯æ•°é‡')
    args = parser.parse_args()
    
    analyzer = PaperAnalyzer()
    analyzer.run(args.keywords, args.max)
